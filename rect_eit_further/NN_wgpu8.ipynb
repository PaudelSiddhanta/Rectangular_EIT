{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the same code as that for NN_wgpu7 but with additional modifications. Until now, we were able to attain good loss for until n=6 but we did not achieve very good results for n=7. So, some changes will be made here.\n",
    "- First change is to log the parameters of the model not just at the end of the model training but also at the point where it had the best loss. I wish to compare the parameters for the best loss and for the end of the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For N=8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rect_grid import Node, GridStructure\n",
    "import solve_linear as sl\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "from rect_NN_class import SquareEITNN\n",
    "from rect_NN_helper import CustomCosineAnnealingWarmRestarts, generate_dtn_data,loss_function,adam_with_grad_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please ensure a GPU is available.\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Running on GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adaptive(num_epochs, dataloader, model, alpha, learning_rate, grid, min_lr=1e-10, eta_max=0.08,\n",
    "                   beta1=0.9, beta2=0.999, eps=1e-8, patience=7500, init_delta_thresh=1e-4, cycle_length=5000):\n",
    "    model.to(device)\n",
    "    adam_states = []\n",
    "    model_grad = [model.W1__grad, model.W2__grad]\n",
    "    model_param = [model.W1, model.Beta]\n",
    "    name = ['W1', 'Beta']\n",
    "    for i, (param, grad) in enumerate(zip(model_param, model_grad)):\n",
    "        m_t = torch.zeros_like(param).to(device)\n",
    "        v_t = torch.zeros_like(param).to(device)\n",
    "        adam_states.append({'name': name[i], 'param': param, 'm': m_t, 'v': v_t, 'param_grad': grad})\n",
    "\n",
    "    scheduler = CustomCosineAnnealingWarmRestarts(T_0=cycle_length, initial_lr=learning_rate, T_mult=1, eta_min=min_lr, eta_max_factor=eta_max)\n",
    "    t = 1\n",
    "    best_loss = float('inf')\n",
    "    best_lr = learning_rate\n",
    "    epochs_since_improvement = 0\n",
    "    beta1_current = beta1\n",
    "    beta2_current = beta2\n",
    "    beta_updated = False\n",
    "\n",
    "    loss_record = []\n",
    "    interior_loss_record = []\n",
    "    boundary_loss_record = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        interior_loss_epoch = 0.0\n",
    "        boundary_loss_epoch = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_x in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            model.zero_grad()\n",
    "            output, hid = model(batch_x)\n",
    "            total_loss, interior_loss, boundary_loss = loss_function(output, grid, alpha)\n",
    "\n",
    "            #assign the gradients manually\n",
    "            model.assign_gradients(batch_x, hid, output, alpha)\n",
    "\n",
    "            #update the parameters usign the custom adam optimizer\n",
    "\n",
    "            for state in adam_states:\n",
    "                param = state['param']\n",
    "                grad = state['param_grad']\n",
    "                m = state['m']\n",
    "                v = state['v']\n",
    "                new_param, new_m, new_v = adam_with_grad_clip(param, grad, learning_rate, m, v, beta1_current, beta2_current, eps, t, max_grad_norm=1.0)\n",
    "                param.data = new_param.data\n",
    "                state['m'] = new_m\n",
    "                state['v'] = new_v\n",
    "            t += 1\n",
    "            model.symmetrize_W2_after_training()\n",
    "\n",
    "            total_loss_epoch += total_loss.item()\n",
    "            interior_loss_epoch += interior_loss.item()\n",
    "            boundary_loss_epoch += boundary_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        total_loss_epoch /= num_batches\n",
    "        interior_loss_epoch /= num_batches\n",
    "        boundary_loss_epoch /= num_batches\n",
    "        loss_record.append(total_loss_epoch)\n",
    "        interior_loss_record.append(interior_loss_epoch)\n",
    "        boundary_loss_record.append(boundary_loss_epoch)\n",
    "\n",
    "        #update alpha as time goes on...initially we started with a lower alpha to prioritize boundary loss but as we get better loss, we also \n",
    "        # give priority to the interior loss\n",
    "\n",
    "\n",
    "        min_loss = min(interior_loss_epoch, boundary_loss_epoch)\n",
    "        if min_loss < 1e-9 and alpha < 1.0:\n",
    "            alpha = 1.0\n",
    "        elif min_loss < 1e-8 and alpha < 0.9:\n",
    "            alpha = 0.9\n",
    "        elif min_loss < 1e-7 and alpha < 0.8:\n",
    "            alpha = 0.8\n",
    "\n",
    "        learning_rate = max(scheduler.step(), min_lr)\n",
    "\n",
    "        #if the base_learning rate becomes too low, reset the base_learning rate andd restart once\n",
    "        # in case we are stuck in a local minima, this should help us get out of it\n",
    "\n",
    "        if scheduler.base_lr < 1e-7 and not beta_updated:\n",
    "            scheduler.reset(new_base_lr=1e-4)\n",
    "            learning_rate = scheduler.step()\n",
    "            beta1_current = 0.95\n",
    "            beta2_current = 0.9999\n",
    "            beta_updated = True\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Resetting Base LR to 1e-4, beta1 to {beta1_current}, beta2 to {beta2_current}\")\n",
    "\n",
    "        if total_loss_epoch < best_loss - init_delta_thresh:\n",
    "            best_loss = total_loss_epoch\n",
    "            best_lr = learning_rate\n",
    "            #reduce the delta_threshold as the training loop progresses\n",
    "            if best_loss > 0:\n",
    "                order = np.floor(np.log10(best_loss))\n",
    "                init_delta_thresh = 1e-4 * 10 ** order\n",
    "            epochs_since_improvement = 0\n",
    "        elif best_loss == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss_epoch:.6e}, Stopping (best_loss = 0)\")\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Total Loss: {total_loss_epoch:.6e}, Interior: {interior_loss_epoch:.6e}, Boundary: {boundary_loss_epoch:.6e}, Alpha: {alpha:.4f}, LR: {learning_rate:.6e}, Best LR: {best_lr:.6e}, Base LR: {scheduler.base_lr:.6e}, Cycle: {scheduler.cycle}, T_cur: {scheduler.T_cur}, Delta Thresh: {init_delta_thresh:.6e}, Beta1: {beta1_current:.4f}, Beta2: {beta2_current:.4f}\")\n",
    "\n",
    "        if total_loss_epoch < 1e-12:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss_epoch:.6e}, Stopping (loss < 1e-12)\")\n",
    "            break\n",
    "\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss_epoch:.6e}, Stopping (no improvement)\")\n",
    "            break\n",
    "\n",
    "    recovered_gamma = model.W2_dynamic.detach().cpu().numpy()\n",
    "    estimated_conductivities = {}\n",
    "    for edge in grid.edges:\n",
    "        p_idx, q_idx = edge\n",
    "        if p_idx >= grid.node_count - grid.n:\n",
    "            continue\n",
    "        estimated_conductivities[edge] = recovered_gamma[p_idx, q_idx]\n",
    "\n",
    "    original_conductivities = copy.deepcopy(grid.conductivities)\n",
    "    print(\"\\nConductivity Comparison Table\")\n",
    "    print(f\"{'Edge':>10} | {'Estimated':>12} | {'True':>12}\")\n",
    "    print(\"-\" * 38)\n",
    "    for edge in grid.edges:\n",
    "        if edge in estimated_conductivities:\n",
    "            est = estimated_conductivities[edge]\n",
    "            true = original_conductivities[edge]\n",
    "            print(f\"{str(edge):>10} | {est.item():12.6f} | {true.item():12.6f}\")\n",
    "\n",
    "    demo_grid = GridStructure(grid.n)\n",
    "    demo_grid.conductivities = original_conductivities\n",
    "    print(\"Original Conductivity\")\n",
    "    demo_grid.visualize_network()\n",
    "\n",
    "    demo_grid2 = GridStructure(grid.n)\n",
    "    demo_grid2.conductivities = estimated_conductivities\n",
    "    print(\"Recovered Conductivity\")\n",
    "    demo_grid2.visualize_network()\n",
    "\n",
    "    plt.plot(loss_record, label=\"Total Loss\")\n",
    "    plt.plot(interior_loss_record, label=\"Interior Loss\")\n",
    "    plt.plot(boundary_loss_record, label=\"Boundary Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return loss_record, interior_loss_record, boundary_loss_record, estimated_conductivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n = 7\n",
    "num_epochs = 30000\n",
    "batch_size = 32\n",
    "learning_rate = 0.02\n",
    "alpha = 0.7\n",
    "cycle_length = 5000\n",
    "\n",
    "grid = GridStructure(n)\n",
    "batch_size = min(4*n, batch_size)\n",
    "data = generate_dtn_data(grid, batch_size)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "model = SquareEITNN(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_record, interior_loss, boundary_loss, estimated_conductivities = train_adaptive(\n",
    "    num_epochs, dataloader, model, alpha, learning_rate, grid,\n",
    "    min_lr=1e-10, eta_max=0.08, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "    patience=1.5*cycle_length, init_delta_thresh=1e-4, cycle_length=cycle_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "n = 7\n",
    "num_epochs = 30000\n",
    "batch_size = 32\n",
    "learning_rate = 0.02\n",
    "alpha = 0.7\n",
    "cycle_length = 5000\n",
    "\n",
    "grid = GridStructure(n)\n",
    "batch_size = min(4*n, batch_size)\n",
    "data = generate_dtn_data(grid, batch_size)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "model = SquareEITNN(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_record, interior_loss, boundary_loss, estimated_conductivities = train_adaptive(\n",
    "    num_epochs, dataloader, model, alpha, learning_rate, grid,\n",
    "    min_lr=1e-14, eta_max=0.08, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "    patience=1.5*cycle_length, init_delta_thresh=1e-4, cycle_length=cycle_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_record, label=\"Total Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
